<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SPIKE-RL: Video-LLMs Meet Bayesian Surprise</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Charter', 'Georgia', serif;
            line-height: 1.7;
            color: #1a1a1a;
            background: #fafafa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 60px rgba(0,0,0,0.08);
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 80px 50px 70px;
            text-align: center;
        }

        h1 {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 2.8em;
            margin-bottom: 24px;
            font-weight: 700;
            letter-spacing: -0.02em;
            line-height: 1.2;
        }

        .subtitle {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 1.2em;
            margin-bottom: 35px;
            opacity: 0.95;
            font-weight: 400;
            line-height: 1.5;
        }

        .meta {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.95em;
            margin-bottom: 12px;
            opacity: 0.9;
            line-height: 1.8;
        }

        .affiliation {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.9em;
            opacity: 0.85;
            margin-bottom: 35px;
        }

        .links {
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 30px;
        }

        .btn {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 12px 24px;
            background: rgba(255, 255, 255, 0.95);
            color: #667eea;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.9em;
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.25);
            background: white;
        }

        article {
            padding: 60px 50px 80px;
        }

        h2 {
            font-family: 'Charter', 'Georgia', serif;
            font-size: 2em;
            margin: 60px 0 30px 0;
            color: #1a1a1a;
            font-weight: 700;
            letter-spacing: 0;
        }

        h2:first-child {
            margin-top: 0;
        }

        h3 {
            font-family: 'Charter', 'Georgia', serif;
            font-size: 1.4em;
            margin: 45px 0 20px 0;
            color: #2a2a2a;
            font-weight: 600;
        }

        p {
            margin-bottom: 24px;
            font-size: 1.1em;
            line-height: 1.8;
            color: #1a1a1a;
        }

        .lead {
            font-size: 1.25em;
            line-height: 1.7;
            color: #2a2a2a;
            margin-bottom: 30px;
        }

        .figure {
            margin: 50px -50px;
            text-align: center;
        }

        .figure img {
            width: 100%;
            border-radius: 0;
            box-shadow: none;
        }

        .figure video {
            width: 100%;
            border-radius: 0;
            box-shadow: none;
        }

        .caption {
            margin-top: 16px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.95em;
            line-height: 1.6;
            color: #666;
            padding: 0 50px;
        }

        .callout {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 30px;
            margin: 40px 0;
            font-size: 1.05em;
            line-height: 1.7;
        }

        .callout strong {
            color: #667eea;
            font-weight: 600;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .stat-box {
            text-align: center;
            padding: 30px 20px;
            background: #f8f9fa;
            border-radius: 8px;
        }

        .stat-number {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 2.5em;
            font-weight: 700;
            color: #667eea;
            margin-bottom: 8px;
        }

        .stat-label {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.95em;
            color: #666;
            font-weight: 500;
        }

        .stat-detail {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.85em;
            color: #888;
            margin-top: 8px;
        }

        ul, ol {
            margin: 24px 0 24px 30px;
            font-size: 1.1em;
            line-height: 1.8;
        }

        li {
            margin-bottom: 12px;
        }

        strong {
            font-weight: 600;
            color: #1a1a1a;
        }

        .bibtex-section {
            background: #f8f9fa;
            padding: 40px 50px;
            margin-top: 60px;
        }

        .bibtex-section h2 {
            margin-top: 0;
        }

        .bibtex {
            background: #1a1a1a;
            color: #e8e8e8;
            padding: 24px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'SF Mono', 'Monaco', 'Courier New', monospace;
            font-size: 0.85em;
            line-height: 1.6;
        }

        footer {
            background: #fafafa;
            padding: 40px 50px;
            text-align: center;
            border-top: 1px solid #e8e8e8;
        }

        footer p {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.9em;
            color: #666;
            margin-bottom: 8px;
        }

        @media (max-width: 768px) {
            .container {
                box-shadow: none;
            }

            header {
                padding: 60px 30px 50px;
            }

            h1 {
                font-size: 2em;
            }

            article {
                padding: 40px 30px 60px;
            }

            .figure {
                margin-left: -30px;
                margin-right: -30px;
            }

            .caption {
                padding: 0 30px;
            }

            .bibtex-section {
                padding: 40px 30px;
            }

            footer {
                padding: 40px 30px;
            }
        }

        .icon {
            width: 18px;
            height: 18px;
        }

        em {
            font-style: italic;
        }

        .task-results {
            margin: 35px 0;
            background: #fafafa;
            border-radius: 8px;
            padding: 30px;
        }

        .task-item {
            padding: 20px 0;
            border-bottom: 1px solid #e8e8e8;
            display: grid;
            grid-template-columns: 1fr 2fr auto;
            gap: 20px;
            align-items: center;
        }

        .task-item:last-child {
            border-bottom: none;
        }

        .task-name {
            font-family: 'Charter', 'Georgia', serif;
            font-weight: 700;
            font-size: 1.1em;
            color: #1a1a1a;
        }

        .task-desc {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 0.95em;
            color: #666;
            line-height: 1.5;
        }

        .task-gain {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-weight: 700;
            font-size: 1.2em;
            color: #667eea;
            text-align: right;
            white-space: nowrap;
        }

        @media (max-width: 768px) {
            .task-item {
                grid-template-columns: 1fr;
                gap: 8px;
            }

            .task-gain {
                text-align: left;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>SPIKE-RL: Video-LLMs Meet Bayesian Surprise</h1>
            <div class="subtitle">Proactive Belief Tracking and Surprise-Aware Frame Sampling for Video Understanding</div>
            
            <div class="meta">
                Sahithya Ravi<sup>1,2</sup>, 
                Aditya Chinchure<sup>1,2</sup>, 
                Raymond T. Ng<sup>1</sup>, 
                Leonid Sigal<sup>1,2</sup>, 
                Vered Shwartz<sup>1,2</sup>
            </div>
            
            <div class="affiliation">
                <sup>1</sup>The University of British Columbia &nbsp;&nbsp; <sup>2</sup>Vector Institute for AI
            </div>

            <div class="links">
                <a href="https://arxiv.org/abs/2509.23433" class="btn">
                    <svg class="icon" fill="currentColor" viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
                    Paper
                </a>
                <a href="https://github.com/sahithyaravi/SPIKE-RL" class="btn">
                    <svg class="icon" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
                    Code
                </a>
                <a href="#citation" class="btn">
                    <svg class="icon" fill="currentColor" viewBox="0 0 24 24"><path d="M7 18c-1.1 0-1.99.9-1.99 2S5.9 22 7 22s2-.9 2-2-.9-2-2-2zM1 2v2h2l3.6 7.59-1.35 2.45c-.16.28-.25.61-.25.96 0 1.1.9 2 2 2h12v-2H7.42c-.14 0-.25-.11-.25-.25l.03-.12.9-1.63h7.45c.75 0 1.41-.41 1.75-1.03l3.58-6.49c.08-.14.12-.31.12-.48 0-.55-.45-1-1-1H5.21l-.94-2H1zm16 16c-1.1 0-1.99.9-1.99 2s.89 2 1.99 2 2-.9 2-2-.9-2-2-2z"/></svg>
                    BibTeX
                </a>
            </div>
        </header>

        <article>
            <p class="lead">
                Real-world videos are filled with routine activities punctuated by memorable, surprising events. Yet most Video-LLMs process videos by uniformly sampling frames, likely missing the critical moments that define a video's narrative.
            </p>

            <p>
                We introduce <strong>SPIKE</strong>, an inference-time framework that quantifies Bayesian Surprise as the belief update triggered by new visual evidence in the video stream. By identifying moments where new observations conflict with prior expectations, SPIKE naturally discovers the most salient and informative frames. Since zero-shot Video-LLM beliefs are often suboptimal, we develop <strong>SPIKE-RL</strong>, which leverages group relative policy optimization (GRPO) to refine belief hypotheses based on caption quality as a reward signal.
            </p>

            <div class="figure">
                <img src="assets/images/teaser.png" alt="SPIKE Overview">
                <div class="caption">
                    SPIKE identifies surprising moments by tracking belief shifts when new frames are observed. Unlike uniform sampling (a), our surprise-based sampling (b) focuses computational resources on high-surprise regions that naturally align with human attention.
                </div>
            </div>

            <p>
                SPIKE and SPIKE-RL enable query-agnostic, surprise-weighted frame sampling that allocates more frames to interesting moments. This strategy yields consistent performance gains across five downstream benchmarks compared to uniform sampling baselines.
            </p>

            <h2>Bayesian Belief Tracking in Action</h2>

            <p>
                At the core of SPIKE is a principled approach to modeling uncertainty and surprise. As the model processes each frame, it maintains explicit probability distributions over possible future events, represented as human-interpretable textual hypotheses. When new visual evidence arrives, these beliefs are updated according to Bayes' rule, and the magnitude of this update—measured via KL divergence—quantifies surprise.
            </p>

            <div class="figure">
                <video controls>
                    <source src="assets/videos/belief_tracking_video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="caption">
                    Visualization of belief tracking dynamics: the model generates hypotheses about upcoming events, computes prior and posterior probability distributions, and registers surprise when expectations are violated. High-surprise moments receive increased attention during frame sampling.
                </div>
            </div>

            <div class="callout">
                <strong>Key insight:</strong> By representing beliefs as distributions over textual hypotheses rather than implicit feature representations, SPIKE provides interpretable explanations for why certain moments are deemed surprising. This transparency is crucial for understanding model behavior and debugging failure cases.
            </div>

            <h2>Method</h2>

            <h3>SPIKE: Inference-Time Surprise Quantification</h3>

            <p>
                SPIKE operates through a four-stage pipeline at each timestep:
            </p>

            <ol>
                <li><strong>Hypothesis generation:</strong> Given recent frames and historical context, generate a diverse set of hypotheses about potential future events</li>
                <li><strong>Prior computation:</strong> Score each hypothesis before observing the next frame to obtain a prior belief distribution</li>
                <li><strong>Posterior update:</strong> Re-score hypotheses after observing the new frame to compute posterior beliefs</li>
                <li><strong>Surprise measurement:</strong> Calculate KL divergence between posterior and prior distributions as the surprise signal</li>
            </ol>

            <div class="figure">
                <img src="assets/images/main.png" alt="SPIKE Architecture">
                <div class="caption">
                    SPIKE architecture showing the hypothesis generator and scorer modules. The generator produces belief hypotheses while the scorer computes likelihood ratios that yield prior and posterior distributions, ultimately producing per-frame surprise scores.
                </div>
            </div>

            <p>
                This formulation has several advantages. First, surprise is computed in a model-agnostic manner that doesn't require gradient access or architectural modifications. Second, the textual hypothesis space enables rich reasoning about complex events. Third, the probabilistic framework provides theoretical grounding and interpretability.
            </p>

            <h3>SPIKE-RL: Reinforcement Learning for Belief Optimization</h3>

            <p>
                A critical challenge is that zero-shot Video-LLMs often generate suboptimal belief hypotheses. To address this, SPIKE-RL employs GRPO to optimize the hypothesis generator. The key observation is that <strong>accurate intermediate beliefs are necessary for producing high-quality final captions</strong>. By using caption quality as a reward signal, we can implicitly supervise the belief tracking process without requiring ground-truth surprise annotations.
            </p>

            <p>
                The training procedure samples multiple belief trajectories per video, computes rewards based on caption quality (using GPT-4 as an evaluator), and updates the generator to increase the probability of high-reward trajectories. This approach yields hypotheses that are both more diverse and better aligned with human judgments of surprise.
            </p>

            <h2>Results</h2>

            <h3>Surprise Localization</h3>

            <p>
                We evaluate SPIKE's ability to identify surprising moments on three benchmarks containing videos with unexpected events:
            </p>

            <div class="stats-grid">
                <div class="stat-box">
                    <div class="stat-number">62.9%</div>
                    <div class="stat-label">Oops! Dataset</div>
                    <div class="stat-detail">Acc@0.25s (vs. 62.1% human)</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">68.2%</div>
                    <div class="stat-label">FunQA Dataset</div>
                    <div class="stat-detail">IoU score</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">61.1%</div>
                    <div class="stat-label">Mr. Bean Dataset</div>
                    <div class="stat-detail">IoU on multi-surprise</div>
                </div>
            </div>

            <p>
                On the Oops! dataset, SPIKE approaches human-level performance at localizing failure events. On FunQA, our method significantly outperforms prior approaches. We also introduce a new Mr. Bean benchmark containing videos with multiple surprising moments, where SPIKE achieves strong performance despite the increased complexity.
            </p>

            <h3>Downstream Task Performance</h3>

            <p>
                To validate that surprise-weighted sampling improves general video understanding, we evaluate on five diverse benchmarks spanning different task types and domains. In each case, we compare uniform frame sampling against our surprise-weighted approach while keeping all other factors constant.
            </p>

            <div class="task-results">
                <div class="task-item">
                    <div class="task-name">BlackSwan</div>
                    <div class="task-desc">Anomalous event detection in long-form videos</div>
                    <div class="task-gain">+2.3% accuracy</div>
                </div>
                <div class="task-item">
                    <div class="task-name">FunQA</div>
                    <div class="task-desc">Understanding creative and humorous video moments</div>
                    <div class="task-gain">+4.6% LLM-Match</div>
                </div>
                <div class="task-item">
                    <div class="task-name">ExFunTube</div>
                    <div class="task-desc">Generating explanations for unexpected outcomes</div>
                    <div class="task-gain">+7.0% LLM-Match</div>
                </div>
                <div class="task-item">
                    <div class="task-name">VideoMME-S</div>
                    <div class="task-desc">Short video comprehension and question answering</div>
                    <div class="task-gain">+2.7% accuracy</div>
                </div>
                <div class="task-item">
                    <div class="task-name">NextQA</div>
                    <div class="task-desc">Causal and temporal reasoning about events</div>
                    <div class="task-gain">+1.7% accuracy</div>
                </div>
            </div>

            <p>
                The consistent improvements across all benchmarks demonstrate that surprise-weighted sampling captures something fundamental about video structure. Notably, gains are substantial even on tasks that don't explicitly focus on surprising events—suggesting that informative moments tend to be surprising moments. The particularly strong results on ExFunTube (+7.0%) highlight how crucial it is to observe the unexpected events that require explanation.
            </p>

            <p>
                These results validate a key hypothesis: by concentrating computational resources on high-information frames, we can achieve better video understanding without increasing the total number of frames processed. This has important implications for scaling video models to longer content.
            </p>

            <div class="callout">
                <strong>Impact of reinforcement learning:</strong> SPIKE-RL improves hypothesis diversity by 6.8% over SPIKE and achieves 0.87 Spearman correlation with human surprise judgments. This demonstrates that RL training enhances not just task performance but the quality of internal model representations.
            </div>

            <h2>Discussion</h2>

            <p>
                SPIKE represents a step toward more human-like video understanding, where models actively predict upcoming events and register surprise when predictions fail. This predictive processing framework has deep connections to theories of human cognition and provides a principled approach to attention allocation in sequential data.
            </p>

            <p>
                Future work could extend SPIKE to longer videos, incorporate multimodal signals beyond vision, and explore whether surprise-based learning can improve model capabilities more broadly. The interpretable nature of textual belief hypotheses also opens opportunities for human-in-the-loop refinement and model debugging.
            </p>
        </article>

        <div class="bibtex-section" id="citation">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <pre class="bibtex">@article{ravi2025spike,
  title={SPIKE-RL: Video-LLMs Meet Bayesian Surprise},
  author={Ravi, Sahithya and Chinchure, Aditya and Ng, Raymond T. and Sigal, Leonid and Shwartz, Vered},
  journal={arXiv preprint arXiv:2509.23433},
  year={2025}
}</pre>
        </div>

        <footer>
            <p>&copy; 2025 University of British Columbia & Vector Institute for AI</p>
            <p>For questions, please contact: sahiravi@cs.ubc.ca</p>
        </footer>
    </div>
</body>
</html>